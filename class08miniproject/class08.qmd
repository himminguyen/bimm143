---
title: "Class 8 : mini project"
author: "Jimmi Nguyen"
format: pdf
---

Loading data using `read.csv`

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```

Remove the diagnosis column and keep it in a separate vector for later.
```{r}
diagnosis = as.factor(wisc.df[, 1])
wisc.data = wisc.df[,-1]
head(wisc.data)
```

## Exploratory data analysis

The first step of any data analysis, unsupervised or supervised, is to familiarize yourself with the data.

>Q1. How many observations are in this dataset?

There are 569 observations in the data set.
```{r}
nrow(wisc.data)
```

>Q2. How many of the observations have a malignant diagnosis?

212 observations are malignant.
```{r}
table(wisc.df$diagnosis)
```

>Q3. How many variables/features in the data are suffixed with _mean?

There are 10 variables with "_mean".

First find the column names
```{r}
colnames(wisc.data)
```

Next I need to search within the column names for "_mean" pattern. The `grep()` function might

```{r}
length(grep("_mean",colnames(wisc.data)))
```

> Q. How many dimensions are in this dataset?

```{r}
ncol(wisc.data)
```

# Principal Component Analysis

First do we need to scale the data before PCA or not.

```{r}
round( apply( wisc.data, 2, sd), 3)
```

Looks like we need to scale

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp( wisc.data, scale=T )
summary(wisc.pr)
```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs capture 72%

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs capture 91%

## PC plot

We need to make our plot of PC1 vs PC2 (a.k.a score plot, )

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

What stands out is all the data points are marked by the id number and the lines are marked with the parameters in the data frame. It is difficult to understand because the graph is too condensed with data names.

```{r}
biplot(wisc.pr)
```

lets generate a more standard scatter plot of each observation along principal components 1 and 2 

```{r}
# Scatter plot observations by components 1 and 2
plot( wisc.pr$x[, 1], wisc.pr$x[, 2], col=diagnosis , 
     xlab = "PC1", ylab = "PC2")
```

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

There is a less defined distinction or grouping between benign and malignant observations than between PC1 and PC2
```{r}
plot( wisc.pr$x[, 1], wisc.pr$x[, 3], col=diagnosis , 
     xlab = "PC1", ylab = "PC3")
```

we can use the ggplot2 package to make a more fancy figure of these results.

```{r}
library(ggplot2)

pc <- as.data.frame(wisc.pr$x)
pc$diagnosis <- diagnosis

ggplot(pc) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 0.5), type = "o")
```

# Examine the PC loadings

How do the original variables contribute to the new PCs that we have calculated? To get at this data we can look at the `$rotation`
component of the returned PCA object.

```{r}
head(wisc.pr$rotation[,1:3])
```

Focus in on PC1
```{r}
head(wisc.pr$rotation[,1])
```

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```

>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

5 principle components are required to explain 80% of the variance of the data.
```{r}
sum(pve[1:5])
```

There is a complicated mix of variables that go together to make up PC1 - i.e. there many of the original variables that together contribute highly to PC1.

```{r}
loadings = as.data.frame(wisc.pr$rotation)

ggplot(loadings) +
  aes(PC1, rownames(loadings)) +
  geom_col()
```

#Hierarchical clustering

>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

At the height of 19.5, the clustering model has 4 clusters.
```{r}
wisc.hclust <- hclust(dist(scale(wisc.data)))
plot(wisc.hclust)
plot(wisc.hclust) + abline(h=19.5, col="red", lty=2)
```

>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

The best match is by cutting into 4 clusters using the `cutree()` function.
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

The method "ward.D2" is my favorite because it results in the cleanest cluster generation while the other methods are signficantly more chaotic.

```{r}
wisc.km <- kmeans(scale(wisc.data), centers= 2, nstart= 20)
table(wisc.km$cluster, diagnosis)
```

>Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

The k-means separate the two diagnoses quite well and it creates a better distinction between the two than the hclust results.

```{r}
table(wisc.hclust.clusters,wisc.km$cluster)
```

Cut this tree to yield cluster membership vector with `cutree()` function.

```{r}
grps = cutree(wisc.hclust, h=19)
table(grps)
```

#Combine methods: PCA and HCLUST

My pca results were interesting as they showed a separation of M and B samples along PC1.

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```

I want to cluster my PCA results - that is use `wisc.pr$x` as input to `hclust()`

```{r}
d = dist(wisc.pr$x[,1:3])

wisc.pr.hclust = hclust(d, method="ward.D2")
```

And my tree results.

```{r}
plot(wisc.pr.hclust)
```

Let's cut the tree into two groups/clusters

```{r}
grps = cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

Cut this hierarchical clustering model into 2 clusters 

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
table(grps, diagnosis)
```

>Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```







